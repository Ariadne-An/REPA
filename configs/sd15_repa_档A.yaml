# U-REPA + SD-1.5 Configuration - Track A (Quick Validation)
# 200k samples, 60k steps, ~2 hours on A100

# ============================================================================
# Experiment Metadata
# ============================================================================
experiment_name: "sd15_repa_档A"
seed: 42

# ============================================================================
# Data Paths
# ============================================================================
csv_path: "/workspace/data/train_200k.csv"
latent_dir: "/workspace/data/vae_latents_lmdb"
dino_dir: "/dev/shm/dino_tokens_lmdb"
clip_embeddings_path: "/workspace/data/clip_embeddings_1001.pt"

# ============================================================================
# VAE Configuration
# ============================================================================
latent_scale: 0.18215  # SD-VAE scaling factor (CRITICAL: must match preprocessing)

# ============================================================================
# DINO Configuration
# ============================================================================
dino_model: "dinov2_vitl14"  # DINOv2 ViT-L/14
dino_transform_id: "dinov2_vitl14_official"  # Official transforms version
dino_grid: 16  # DINO token grid size (16x16)
dino_num_tokens: 256  # Number of tokens per sample
align_layers: ["mid"]  # Alignment layers: ["mid"] or ["enc_last", "mid", "dec_first"]
dino_D: 1024  # DINOv2-L dimension

# ============================================================================
# Scheduler Configuration
# ============================================================================
schedule:
  name: "linear"  # Options: "linear" or "cosine"
  parametrization: "v"  # Options: "v" or "epsilon"
  weighting: "uniform"  # Options: "uniform" or "lognormal"

# ============================================================================
# LoRA Configuration
# ============================================================================
use_lora: true
lora_rank: 8
lora_alpha: 8
lora_targets: "attn"  # Attention-only for mid-block self-attn
lora_train_layers:
  - enc_mid
  - enc_last
  - mid
  - dec_first
  - dec_second
lora_lr: 1.0e-4
lora_weight_decay: 0.0
align_head_lr: 1.0e-4
align_head_weight_decay: 0.01

# ============================================================================
# Loss Configuration
# ============================================================================
align_coeff: 0.5  # Final λ after warmup
align_warmup_steps: 3000  # Linear ramp from 0 → λ
manifold_coeff: 3.0  # w: weight for manifold loss (relative to token loss)
manifold_upper_only: true  # Use upper triangle only in Gram matrix
manifold_mask_diag: true  # Mask diagonal in Gram matrix

# ============================================================================
# Training Configuration
# ============================================================================
batch_size: 128
max_steps: 60000  # Used if epoch_count is not provided
epoch_count: 40  # Preferred: overrides max_steps based on dataset length
learning_rate: 1.0e-4
weight_decay: 0.01
ema_decay: 0.9995
cfg_dropout: 0.1  # Label dropout probability for CFG
gradient_clip: 1.0
mixed_precision: "bf16"  # Options: "no", "fp16", "bf16"
num_workers: 8

# ============================================================================
# Optimization Schedule
# ============================================================================
warmup_ratio: 0.1  # Warmup steps = warmup_ratio * max_steps
lr_schedule: "cosine"  # Options: "cosine", "linear", "constant"

# ============================================================================
# Logging & Evaluation
# ============================================================================
log_interval: 100  # Log every N steps
eval_interval: 5000  # Evaluate every N steps
save_interval: 6000  # Save checkpoint every N steps (≈4 epochs)
val_csv_path: "/workspace/data/val_50k.csv"
val_latent_dir: "/workspace/data/val_vae_latents_lmdb_fixed"
val_dino_dir: "/workspace/data/val_dino_tokens_lmdb"
val_clip_embeddings_path: "/workspace/data/clip_embeddings_1001.pt"
val_batch_size: 64
val_num_batches: 50
val_interval: 3000
val_repeat: 3
val_cfg_dropout: 0.1
local_log_path: "logs/trackA_train.jsonl"
report_to: "wandb"  # Options: "wandb", "tensorboard", "none"

# FID convergence targets (for plotting)
eval_fid_targets: [5.0, 3.0, 2.0, 1.5]

# Evaluation settings
eval_num_samples: 10000  # Number of samples for FID calculation
eval_batch_size: 64
eval_cfg_scale: 7.5  # CFG scale for sampling
eval_num_inference_steps: 250  # NFE (Number of Function Evaluations)
eval_guidance_high: 0.7  # Upper bound for guidance interval

# ============================================================================
# Output Configuration
# ============================================================================
output_dir: "exps"
checkpoint_dir: "checkpoints"
log_dir: "logs"

# ============================================================================
# Model Paths
# ============================================================================
pretrained_model_name_or_path: "runwayml/stable-diffusion-v1-5"
vae_name_or_path: "runwayml/stable-diffusion-v1-5"  # Use same VAE
dino_ckpt_path: "checkpoints/dinov2_vitl14.pth"  # Local DINO checkpoint

# ============================================================================
# Resume Training
# ============================================================================
resume_from_checkpoint: null  # Set to checkpoint path to resume
resume_step: 0

# ============================================================================
# Reproducibility
# ============================================================================
# Note: These settings are for reproducibility
# Set cudnn.deterministic = True and cudnn.benchmark = False in code
deterministic: true

# ============================================================================
# Debug Mode
# ============================================================================
debug: false  # Enable debug mode for verbose logging
debug_steps: 10  # Run only N steps in debug mode
